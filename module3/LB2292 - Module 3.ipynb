{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAjxQ2T-i9a2"
   },
   "source": [
    "# Module 3:  Clustering\n",
    "\n",
    "Notebook version: `25.0` (please don't change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The code cell below should automatically download the required data (`cigar.csv`, `easy.csv`, `messy.csv`) and utility functions (`LST_Functions.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1269,
     "status": "ok",
     "timestamp": 1713781078577,
     "user": {
      "displayName": "Timo Verlaan",
      "userId": "18117607379973306750"
     },
     "user_tz": -120
    },
    "id": "SA0XlYCvfJUf",
    "outputId": "16bfb497-4414-49d1-8f13-2996940273dc"
   },
   "outputs": [],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/brmprnk/LB2292/main/module3/cigar.csv\n",
    "!wget -nc https://raw.githubusercontent.com/brmprnk/LB2292/main/module3/easy.csv\n",
    "!wget -nc https://raw.githubusercontent.com/brmprnk/LB2292/main/module3/messy.csv\n",
    "!wget -nc https://raw.githubusercontent.com/brmprnk/LB2292/main/LST_Functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CAKzTIJE2UKs"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from numpy import genfromtxt\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster import hierarchy\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "from LST_Functions import plot_scatter, annotate_distances, kmeans_cluster_vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNvC3815i8tF"
   },
   "source": [
    "## 1.1 Clustering\n",
    "---\n",
    "\n",
    "Clustering is the process of **grouping a set of data objects** into multiple groups or clusters so that objects within a cluster **have high similarity**. It is used as a method to see if natural groupings are present in the data. If these groupings do emerge, these may be named and their properties summarized. All the different clustering methods can produce a partitioning of the dataset. However, different methods will often yield different groupings since each implicitly imposes a structure on the data.\n",
    "\n",
    "### 1.  Hierarchical clustering\n",
    "\n",
    "Hierarchical clustering groups data over a variety of scales by creating a cluster tree or _dendrogram_. The tree is not a single set of clusters, but rather a multilevel hierarchy, where clusters at one level are joined as clusters at the next level. This allows you to decide the level or scale of clustering that is most appropriate for your application.\n",
    "\n",
    "Hierarchical clustering is based on three major steps:\n",
    "\n",
    "1. Finding the **similarity** or **dissimilarity** between every pair of objects in the data set.  In this step, you calculate the distance between objects.\n",
    "\n",
    "2. Grouping the objects into a binary, hierarchical cluster tree (**dendrogram**).  In this step, you link pairs of objects that are in close proximity (have a high similarity) using different linkage methods. The **linkage** method uses the distance information generated in step 1 to determine the similarity of objects to each other. As objects are paired into binary clusters, the newly formed clusters are grouped into larger clusters until a hierarchical tree is formed\n",
    "\n",
    "3. Determine where to **cut** the hierarchical tree into clusters. In this step the obtained dendrogram is **analyzed** in order to decide which cutting level best suits the data.\n",
    "\n",
    "### 1.1 Similarity-Dissimilarity\n",
    "\n",
    "A similarity measure for two objects, i and j will be large when the objects are very similar. On the other hand, a dissimilarity measure will be large when the two objects are very dissimilar and small when they are similar, just like a distance measure between objects. Several distance measures are commonly used for computing the dissimilarity of objects described by numeric attributes.  Below we describe some examples:\n",
    "\n",
    "Let $i = (x_{i1}, x_{i2}, \\dots, x_{in})$ and $j = (x_{j1}, x_{j2}, \\dots, x_{jn})$ be two objects described by $n$ numeric attributes.\n",
    "\n",
    "*   The **Euclidean** distance between the two objects is defined as:\n",
    "$$d(i, j) = \\sqrt{\n",
    "  (x_{i1} - x_{j1})^2 + (x_{i2} - x_{j2})^2 + \\dots + (x_{in} - x_{jn})^2\n",
    "  }$$\n",
    "\n",
    "*   The **Manhattan** distance is defined as:\n",
    "$$ d(i, j) = |x_{i1} - x_{j1}| + |x_{i2} - x_{j2}| + \\dots + |x_{in} - x_{jn}| $$\n",
    "\n",
    "*   To base a distance measure on the **correlation** between the objects, we need to convert the similarity measure into a dissimilarity measure (by negation):\n",
    "$$d(i, j) = 1 - (|x_{i1} * x_{j1}| + |x_{i2} * x_{j2}| + \\dots + |x_{in} * x_{jn}|)$$\n",
    "\n",
    "The module to calculate distances between objects from the Scientific Python (scipy) package is distance. The distance module contains many distance functions which return the distances between two points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJDxOOd83mBG"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 1.\n",
    "\n",
    "> Given x1 and x2 in the figure below, what is the **Euclidean Distance** between x1 and x2?\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "\n",
    "(check the correct box below)\n",
    "\n",
    "- [ ] a) 3.61\n",
    "- [ ] b) 25\n",
    "- [ ] c) 5\n",
    "- [ ] d) 3.1\n",
    "- [ ] e) 4.8\n",
    "\n",
    "> Given x1 and x2 in the figure below, what is the **Manhattan Distance** between x1 and x2?\n",
    "\n",
    "\n",
    "(check the correct box below)\n",
    "\n",
    "- [ ] a) 3.61\n",
    "- [ ] b) 25\n",
    "- [ ] c) 5\n",
    "- [ ] d) 3.1\n",
    "- [ ] e) 4.8\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/brmprnk/LB2292/main/module3/img/ex1.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wGxU72A4Qco"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 2\n",
    "\n",
    "> Below, you are given a toy dataset.\n",
    ">\n",
    "> What are the pairwise Euclidean distances  between all objects of the toy data set from above? To help you to identify the exact distances between two objects you can make use of the function `annotate_distances` from `LST_Functions.py`. \n",
    ">\n",
    "> Hint: The `annotate_distances` function requires the pairwise distances between all points as input, not the datapoints inself. To calculate these distances, you can use the `distance.pdist` function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "executionInfo": {
     "elapsed": 457,
     "status": "ok",
     "timestamp": 1709030111206,
     "user": {
      "displayName": "Timo Verlaan",
      "userId": "18117607379973306750"
     },
     "user_tz": -60
    },
    "id": "zii5F9B5kedC",
    "outputId": "ae1194d4-b1c3-4495-9ced-1357e5652f3d"
   },
   "outputs": [],
   "source": [
    "# Make a small data matrix\n",
    "data = numpy.array([[1, 2], [2.5, 4.5], [2, 2], [4, 1.5], [4, 2.5]])\n",
    "labels = ['A', 'B', 'C', 'D', 'E']\n",
    "# Plot to screen\n",
    "plot_scatter(data, labels)\n",
    "\n",
    "# SOLUTION:\n",
    "\n",
    "# Your code here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fxr9nZ4gkgGd"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 3\n",
    "\n",
    "> **On paper**, build the dendrogram of the hierarchical clustering tree of the 5 data points using average linkage. Make sure to add the height of the bridges on the y-axis.\n",
    ">\n",
    "> You don't have to hand in this exercise, but please check with a TA to see if you did it correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxFRl1Z1ks61"
   },
   "source": [
    "## 1.2 Building the dendrogram - Linkage methods\n",
    "\n",
    "Once the dissimilarity between objects in the data set has been computed, you can determine how objects in the data set should be grouped into clusters by choosing one of the linkage methods. The function `linkage` of the module hierarchy that you imported before takes the distance information generated by `pdist` and links pairs of objects that are close together into binary clusters. The `linkage` function then links these newly formed clusters to each other and to other objects to create bigger clusters until all objects in the original data set are linked together in a hierarchical tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 4a (single-link)\n",
    "\n",
    "> The first linkage function we use is the **single-link** method, where the distance between two groups is defined as the distance between their two closest members.\n",
    "> \n",
    "> Please fill in the missing parts (denoted `???`) in the following code snippets and make use of the distances you've calculated in exercise 2. Inspect the dendrograms. Can you reconstruct the distances between the data points from the dendrograms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "executionInfo": {
     "elapsed": 237,
     "status": "ok",
     "timestamp": 1709030111439,
     "user": {
      "displayName": "Timo Verlaan",
      "userId": "18117607379973306750"
     },
     "user_tz": -60
    },
    "id": "4A2hCDtAlD3A",
    "outputId": "098b5696-7ac3-4729-b698-63d2211ca875"
   },
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "# Replace the ???s in the code below\n",
    "\n",
    "??? = hierarchy.linkage(???, 'single')\n",
    "dn = hierarchy.dendrogram(???, labels=labels,color_threshold=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J61e8XHclOwR"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 4b (complete link)\n",
    "\n",
    "> The second linkage function we investigate is **complete linkage**, where the distance between two groups is defined as the distance between their two farthest-apart members. This method usually yields clusters that are well separated and compact.\n",
    "> \n",
    "> Please fill in the missing parts (denoted `???`) in the following code snippets and make use of the distances you've calculated in exercise 2. Inspect the dendrograms. Can you reconstruct the distances between the data points from the dendrograms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1709030111716,
     "user": {
      "displayName": "Timo Verlaan",
      "userId": "18117607379973306750"
     },
     "user_tz": -60
    },
    "id": "duz6bSMZlYg_",
    "outputId": "19a22c36-f3eb-4015-f3fe-84f363a1194c"
   },
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "# Replace the ???s in the code below\n",
    "\n",
    "??? = hierarchy.linkage(???, 'complete')\n",
    "dn = hierarchy.dendrogram(???,labels=labels,color_threshold=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ApOKIHflbFA"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 4c (average linkage)\n",
    "\n",
    "> The final linkage function we investigate is **average linkage**, which defines the distance between groups as the average distance between each of the members, weighted so that the two groups have an equal influence on the final result.\n",
    "> \n",
    "> Please fill in the missing parts (denoted `???`) in the following code snippets and make use of the distances you've calculated in exercise 2. Inspect the dendrograms. Can you reconstruct the distances between the data points from the dendrograms?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1709030111956,
     "user": {
      "displayName": "Timo Verlaan",
      "userId": "18117607379973306750"
     },
     "user_tz": -60
    },
    "id": "W4A_Hu8Uljcr",
    "outputId": "8ff8ae9b-e082-463d-f7dd-f2759e61232f"
   },
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "# Replace the ???s in the code below\n",
    "\n",
    "??? = hierarchy.linkage(???, 'average')\n",
    "dn = hierarchy.dendrogram(???, labels=labels, color_threshold=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoaNU1EqmsJ1"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "####  \u270f Exercise 5\n",
    "\n",
    "> Suppose you want to group the objects of the toy example into 2 clusters. Depending on the linkage methods you use, how are the objects grouped?\n",
    "\n",
    "| Linkage  | Answer     | cluster 1           | cluster 2       |\n",
    "|----------|------------|-----------------------|-----------------|\n",
    "| Single   | a)         | A C D E               | B               |\n",
    "|          | b)         | A B                   |      C D E      |\n",
    "|          | c)         | A C                   |      D E        |  \n",
    "| Complete | a)         | B C D                 | A E             |\n",
    "|          | b)         | A C D E               |      B          |\n",
    "|          | c)         | A C B                 |      D E        |\n",
    "| Average  | a)         | A C                   | B D E           |\n",
    "|          | b)         | A C D E               |      B          |\n",
    "|          | c)         | A C                   |      B D E      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9Ox4xIe-M_z"
   },
   "source": [
    "Hierarchical clustering produces a dendrogram. To give the desired number of clusters, the tree can be cut at a desired horizontal level. The number of vertical stems of the dendrogram intersected by a horizontal line at the desired level, corresponds to the number of clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBBY1MgC9C4p"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 6\n",
    "\n",
    "> Next, we will load the `cigar` dataset, and cluster by means of euclidean distance and complete linkage.\n",
    ">\n",
    "> Fill in the missing parts of the code  and view the dendrogram. What is the optimal number of clusters (based on your visual inspection of the dendrogram)?\n",
    "> \n",
    "> Select your answer by marking a checkbox below:\n",
    "\n",
    "- [ ] a) 2\n",
    "- [ ] b) 10\n",
    "- [ ] c) 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 842
    },
    "executionInfo": {
     "elapsed": 6399,
     "status": "ok",
     "timestamp": 1709030118351,
     "user": {
      "displayName": "Timo Verlaan",
      "userId": "18117607379973306750"
     },
     "user_tz": -60
    },
    "id": "cKB9SkFmpPHE",
    "outputId": "c88cdfeb-ecd7-4074-e20b-591aa2019181"
   },
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "# Replace the ???s in the code below\n",
    "??? = genfromtxt('cigar.csv', delimiter=',')\n",
    "plot_scatter(???)\n",
    "??? #distance calculations\n",
    "??? #linkage creation\n",
    "dn = hierarchy.dendrogram(???, color_threshold=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0k4TDsepRPG"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 7\n",
    "\n",
    "> How many clusters do you obtain if you cut the dendrogram at 4 ? (Hint: You can play around with the color_threshold parameter)\n",
    "> \n",
    "> Select your answer by marking a checkbox below:\n",
    "\n",
    "\n",
    "- [ ] a) 2\n",
    "- [ ] b) 6\n",
    "- [ ] c) 3\n",
    "- [ ] d) 4\n",
    "- [ ] e) 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2O8fG8pW-iel"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 8\n",
    "\n",
    "> When you cut the dendrogram at 7, you will obtain 2 clusters. What is the complete linkage distances between both clusters.\n",
    "> \n",
    "> Select your answer by marking a checkbox below:\n",
    "\n",
    "Select an answer below by mark\n",
    "- [ ] a) 9.5\n",
    "- [ ] b) 8\n",
    "- [ ] c) 6\n",
    "- [ ] d) 7\n",
    "- [ ] e) 7.8\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bu13PhE9qAH3"
   },
   "source": [
    "### 1.2.4 Hierarchical clustering of the \"Messy data\"\n",
    "\n",
    "Load the messy data set `messy.csv`. This is not a real gene expression matrix, but simply a data set of 300 points in a two dimensional space, e.g. the measurements of 300 genes using two microarrays. The purpose of this exercise is to get you acquainted with clustering gain insight in the different parameters that you can set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 1212,
     "status": "ok",
     "timestamp": 1709030119561,
     "user": {
      "displayName": "Timo Verlaan",
      "userId": "18117607379973306750"
     },
     "user_tz": -60
    },
    "id": "N5nmCzNbrBv6",
    "outputId": "13e555b0-7422-4f78-9c0f-c8bc4b1f12a6"
   },
   "outputs": [],
   "source": [
    "messy = genfromtxt('messy.csv', delimiter=',')\n",
    "plot_scatter(messy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kj5O1Rifq-zA"
   },
   "source": [
    "This plot visualizes 300 genes as 300 dots. In this 2D scatterplot, the X-axis (horizontal) indicates the first measurement and the Y-axis (vertical) indicates the second measurement.\n",
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 9\n",
    "\n",
    "> The *messy* dataset shows two noisy clusters that are almost overlapping. Which linkage works best to obtain the two underlying noisy clusters?\n",
    ">\n",
    "> Select your answer by marking a checkbox below:\n",
    "\n",
    "- [ ] a) single\n",
    "- [ ] b) complete\n",
    "- [ ] c) average\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 10\n",
    "\n",
    "> Using complete linkage, if you cut the euclidean distance dendogram at 1.5. How many clusters do you obtain?\n",
    ">\n",
    "> Select your answer by marking a checkbox below:\n",
    "\n",
    "- [ ] a) 3\n",
    "- [ ] b) 4\n",
    "- [ ] c) 5\n",
    "- [ ] d) 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnKX-Bdorav9"
   },
   "source": [
    "## 2. K-means clustering\n",
    "\n",
    "k\u00admeans clustering is a partitioning method. This method partitions data into k mutually exclusive clusters, and returns the index of the cluster to which it has assigned each observation. Unlike hierarchical clustering, k\u00admeans clustering operates on actual observations (rather than the larger set of dissimilarity measures), and creates a single level of clusters. The Hierarchic clustering method starts from the bottom, and combines observations to clusters. At higher levels it can only fuse existing clusters to others, but it cannot break old groups to build better ones.\n",
    "\n",
    "### 2.1 K-means Algorithm\n",
    "\n",
    "The k-means algorithm defines the centroid of a cluster as the mean value of the points within the cluster. First, it randomly selects k of the objects in the dataset, each of which initially represents a cluster mean or center. For each of the remaining objects, an object is assigned to the cluster to which it is most similar, based on the Euclidean distance between the object and the cluster mean. The k-mean algorithm then iteratively improves the within-cluster variation. For each cluster, it computes the new mean using the objects assigned to the cluster in the previews iteration. All the objects are then reassigned using the updated means as the new cluster centers. The iterations continue until the assignment is stable, that is the clusters formed in the current round are the same as those formed in the previous round.  \n",
    "\n",
    "$k$: the number of clusters\n",
    "\n",
    "$D$: a dataset containing n objects\n",
    "\n",
    "1. arbitrarily choose k objects from D as the initial cluster centers;\n",
    "2. **repeat**\n",
    "  - (re)assign each object to the cluster to which the object is the most similar, based on the mean value of the objects in the cluster;\n",
    "  - update the cluster means;\n",
    "5. **until** no change;\n",
    "\n",
    "The k-means method is not guaranteed to converge to the global optimum and often terminates at a local optimum. The results may depend on the initial random selection of cluster centers. To obtain good results in practice, it is common to run the k-mean algorithm multiple times with different initial cluster centers.\n",
    "\n",
    "The k-means method can be applied only when the mean of a set of objects is defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRpgSjBMBHoV"
   },
   "source": [
    "### 2.2 K-means in Python\n",
    "Load the cigar and messy datasets and visualize them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 843
    },
    "executionInfo": {
     "elapsed": 2643,
     "status": "ok",
     "timestamp": 1709030122200,
     "user": {
      "displayName": "Timo Verlaan",
      "userId": "18117607379973306750"
     },
     "user_tz": -60
    },
    "id": "tSgNKSA0sCCi",
    "outputId": "2a1c5e09-8663-4f63-80a8-18c53a66a9d6"
   },
   "outputs": [],
   "source": [
    "cigar = genfromtxt('cigar.csv', delimiter=',')\n",
    "messy = genfromtxt('messy.csv', delimiter=',')\n",
    "plot_scatter(cigar)\n",
    "plot_scatter(messy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9u-N9RlsKOl"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 11\n",
    "\n",
    "> Based on just the two plots of the data above, what could be a good number of K for each of the data sets?\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "(your answer here)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3RRtF0WCHnZ"
   },
   "source": [
    "In Python you can perform k-means clustering via the KMeans class from the `sklearn.cluster` module. You first have to create a `KMeans` object and then fit data to it. The fitted `KMeans` object can then be visualized in a 2 dimensional scatter plot via the `kmeans_cluster_vis` function which is defined in `LST_Functions.py`. (hint: The input of `kmeans_cluster_vis` is explained in its help report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 1033,
     "status": "ok",
     "timestamp": 1709030123231,
     "user": {
      "displayName": "Timo Verlaan",
      "userId": "18117607379973306750"
     },
     "user_tz": -60
    },
    "id": "ATnAG1NNsO5R",
    "outputId": "749d8b64-ea5d-4025-f9f3-8ce0447f414b"
   },
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "# Replace the ???s in the code below\n",
    "\n",
    "k = ??? # Set the number of clusters\n",
    "kmeans_model = KMeans(n_clusters=k, n_init=\"auto\").fit(messy)\n",
    "kmeans_cluster_vis(messy, kmeans_model.labels_ ,kmeans_model.cluster_centers_, dimensions=(0, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlFxfdDUsRT9"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 12\n",
    "\n",
    "> After running the KMeans function above a few times, what k would you advice for which data set and why?\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "(your answer here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIWdqpxLsOi0"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 13\n",
    "\n",
    "> Is K-means clustering better suited to one of these datasets, and if so, why?\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "(your answer here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sz2t4inxsaTu"
   },
   "source": [
    "## Cluster validation: Davies-Bouldin index\n",
    "\n",
    "The Davies-Bouldin criterion is based on a ratio of within-cluster and between-cluster distances. The Davies-Bouldin index is defined as\n",
    "\n",
    "$$\n",
    "DB = \\frac{1}{k} \\sum_i^{k} \\max_{j \\neq i} \\{D_{i,j}\\}\n",
    "$$\n",
    "\n",
    "where $D_{i,j}$ is the within-to-between cluster distance ratio for the $i$th and $j$th clusters.\n",
    "\n",
    "In mathematical terms, $D_{i,j} = \\frac{\\overline{d_i} + \\overline{d_j}}{d_{i,j}}$\n",
    "\n",
    "$\\overline{d_i}$ is the avarege distance between each point in the $i$th cluster and the centroid of the $i$th cluster.\n",
    "\n",
    "$\\overline{d_j}$ is the avarege distance between each point in the $j$th cluster and the centroid of the $j$th cluster.\n",
    "\n",
    "$d_{i,j}$ is the Euclidean distance between the centroids of the $i$th and $j$th clusters.\n",
    "\n",
    "The maximum value of $D_{i,j}$ represents the worst-case within-to-between cluster ratio for cluster $i$. The optimal clustering solution has the smallest Davies-Bouldin index value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUiQyQhOFzU4"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 14\n",
    "\n",
    "> Load the `easy` data set and evaluate the optimal number of k using the Davies-Bouldin Index and k-means evaluation criterion. Use cluster numbers of 2 to 6.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "(your anwer here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1709030123232,
     "user": {
      "displayName": "Timo Verlaan",
      "userId": "18117607379973306750"
     },
     "user_tz": -60
    },
    "id": "BnfEw7ZNswMN",
    "outputId": "6c2b5b00-6f47-417a-b8de-ad4692ae3b34"
   },
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "# (use the code below)\n",
    "\n",
    "easy = genfromtxt('easy.csv', delimiter=',')\n",
    "kmeans_model = KMeans(n_clusters=2, n_init=\"auto\").fit(easy)\n",
    "davies_bouldin_score(easy, kmeans_model.labels_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4sNf5LAsv2r"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 15\n",
    "\n",
    "> In your lecture you have learned about the Silhouette score. Do you find the same number of optimal clusters when you use the Silhouette score to evaluate your cluster performances? Use cluster numbers of 2 to 6.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "(your answer here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fagJ6u7_HA5X"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 16\n",
    "\n",
    "> What is the purpose of the `silhouette_score` and `silhouette_samples` functions, and what is the difference between them?\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "(your answer here)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1pH0BCkY8nZu6zuq5VGjyiYYhdAT4ZtY8",
     "timestamp": 1705942824857
    }
   ]
  },
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}