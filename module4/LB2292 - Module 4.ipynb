{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsPuEY7rBpw6"
   },
   "source": [
    "# Module 4:  Classification\n",
    "\n",
    "Notebook version: `25.0` (please don't change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Each group should submit to the practical questions on Brightspace.\n",
    "\n",
    "NOTE: For this practical you will need to import the following necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1643,
     "status": "ok",
     "timestamp": 1709904548296,
     "user": {
      "displayName": "Timo Verlaan",
      "userId": "18117607379973306750"
     },
     "user_tz": -60
    },
    "id": "nbBAyXLEtfxq",
    "outputId": "044893dd-7f3c-41de-cd34-b6963ea7334a"
   },
   "outputs": [],
   "source": [
    "# Download helper functions and datasets:\n",
    "!wget -nc https://raw.githubusercontent.com/brmprnk/LB2292/main/module4/cigarsdata.pkl\n",
    "!wget -nc https://raw.githubusercontent.com/brmprnk/LB2292/main/module4/Genesdata.pkl\n",
    "!wget -nc https://raw.githubusercontent.com/brmprnk/LB2292/main/LST_Functions.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(pd.__version__)\n",
    "\n",
    "import pickle\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from LST_Functions import plot_decision_boundary, learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPTrHLcIDJ81"
   },
   "source": [
    "## 1. Bayesian Classification\n",
    "\n",
    "The following figure shows the uniform conditional probability density functions of two classes. The first class $p(x|w_1)$ is indicated by a dashed blue line and the second class $p(x|w_2)$ with a solid black line. The two classes have equal priors: $p(w_1) = p(w_2) = 1/2$.\n",
    "\n",
    "![](https://raw.githubusercontent.com/brmprnk/LB2292/main/module4/img/bayesian.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 1a\n",
    "\n",
    "> Use Bayes rule to derive the class posterior probabilities of the following objects: $x = 3$, $x = -0.54$, $x = 1$, $x = -2$. (i.e. for each object $x$, you want to calculate $p(w_1|x)$ and $p(w_2|x)$).\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "(your solution here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 1b\n",
    "\n",
    "> Based on the calculated posterior probabilities, to which class do you assign each object?\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "(your answer here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 1c\n",
    "\n",
    "> Where do you draw the decision boundary of the Bayes classifier?\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "(your solution here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 1d\n",
    "\n",
    "> Compute the Bayes error.\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "(your solution here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 1e\n",
    "\n",
    "> Assume that the class priors are: $P(w_1) = 1/3$ and $P(w_2) = 2/3$. Recalculate the posterior probabilities for $x = 3$, $x = -0.5$, $x = 1$.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "(your solution here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LG86zCK0x4TH"
   },
   "source": [
    "### 2: Programming a Bayes classifier\n",
    "\n",
    "Based on the two class conditional probabilities of the previous exercise we will generate 200 data points, 100 for each class. The data points generated based on \u03c91 are uniformly distributed ranging between -1 and 2 (x1). The data points generated based on \u03c92 are uniformly distributed ranging between 0 and 4 (x2). The following Python code generates this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uGHgrlwfGH-J"
   },
   "outputs": [],
   "source": [
    "# Generate 100 samples for both classes\n",
    "x1 = np.random.uniform(0, 1, 100)*3 -1 # W1\n",
    "x2 = np.random.uniform(0, 1, 100)*4 # W2\n",
    "\n",
    "# Join the samples into one matrix X\n",
    "\n",
    "X = np.hstack((x1, x2))\n",
    "\n",
    "# Create an array of class labels (1 or 2) for the corresponding data points in X\n",
    "y = np.ones(X.shape[0], int)\n",
    "y[100:] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APj828lwGHhv"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 2a\n",
    "\n",
    "> We just generated the data based on the class conditional probabilities, we know to which of the two classes each data point actually belongs. However, without that foreknowledge we can also use a Bayes classifier to classify each data point and assign it to either \u03c91 or \u03c92. Complete the Python code below to classify all the objects from the dataset using a Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxxNvjUrG03Q"
   },
   "outputs": [],
   "source": [
    "p_w_1 = 0.5\n",
    "p_w_2 = 0.5\n",
    "p_x_w_1 = 1/3\n",
    "p_x_w_2 = 0.25\n",
    "\n",
    "\n",
    "# An array where the predicted class labels are stored\n",
    "y_predicted = np.zeros(X.shape[0], int)\n",
    "\n",
    "# Loop over all objects and classify them\n",
    "for i in range(0, len(X)):\n",
    "  xi = X[i]\n",
    "\n",
    "  # Calculate P(x_i | w_1)\n",
    "  if (xi >= -1) and (xi <= 2):\n",
    "    p_xi_given_w1 = ??? # Replace the ???\n",
    "  else:\n",
    "    p_xi_given_w1 = 0\n",
    "\n",
    "  # Calculate P(x_i | w_2)\n",
    "  if (xi >= 0) and (xi <= 4):\n",
    "    p_xi_given_w2 = ??? # Replace the ???\n",
    "  else:\n",
    "    p_xi_given_w2 = 0\n",
    "\n",
    "  # Multiply prior with class-conditional\n",
    "  bayes_rule_numerator_w1 = ???  # Replace the ???\n",
    "\n",
    "  bayes_rule_numerator_w2 = ???  # Replace the ???\n",
    "\n",
    "  # Calculate P(x_i) (denominator in bayes rule)\n",
    "  p_xi = ??? # Replace the ???\n",
    "\n",
    "  # Apply the bayes rule to calculate the posterior probability of each class\n",
    "  p_w_1_given_xi = ??? # Replace the ???\n",
    "  p_w_2_given_xi = ??? # Replace the ???\n",
    "\n",
    "  # classify object: w1 or w2\n",
    "  if (p_w_1_given_xi > p_w_2_given_xi):\n",
    "    y_predicted[i] = ??? # Replace the ???\n",
    "  else:\n",
    "    ??? # Replace the ???\n",
    "\n",
    "error_rate = ???\n",
    "print(error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 2b\n",
    "\n",
    "> To see how well your Bayes classifier performed, count how many objects from x1 and x2 are misclassified. How does this compare to the Bayes error that you computed in the previous exercise?\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "(your answer here)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1IkNVjDIn10"
   },
   "source": [
    "## 3. Linear Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 3\n",
    "\n",
    "> Load the cigars dataset from the *cigarsdata.pkl* file using the Python code below.\n",
    "> Create a training and test set with `train_test_split`, with 50% test size.\n",
    "> Create a linear discriminant classifier using `LinearDiscriminantAnalysis`, train the classifier using `fit` method.\n",
    "> Make a scatter plot of the data and plot the boundary of the classifier using the `plot_decision_boundary` function given in `LST_Functions` (hint: check the function documentation in order to use it properly).\n",
    "> Finally, obtain the classifier predictions for the test set using predict method and check the classification error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w7rtpMR5G21c"
   },
   "outputs": [],
   "source": [
    "with open('cigarsdata.pkl', 'rb') as f:\n",
    "  datadict = pickle.load(f)\n",
    "\n",
    "data = datadict['data']\n",
    "labels = datadict['labels']\n",
    "del datadict\n",
    "\n",
    "# SOLUTION\n",
    "\n",
    "# Your code here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiyRAWzvJGZV"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 4a\n",
    "\n",
    "> Create two interleaving half circles (banana-shaped) dataset with 400 samples, using the code below.\n",
    ">\n",
    "> Repeat the previous exercise using the new data (`LinearDiscriminantAnalysis`, `fit`, `plot_decision_boundary`, and `predict`). Is the linear classifier appropriate for this problem? What is the error rate in this case?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpkD68pNJLLq"
   },
   "outputs": [],
   "source": [
    "Banana_shaped = datasets.make_moons(n_samples=400, noise=0.05)\n",
    "data = Banana_shaped[0]\n",
    "labels = Banana_shaped[1]\n",
    "\n",
    "# SOLUTION:\n",
    "\n",
    "# Your code here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUZMAUcXJXyo"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 4b\n",
    "\n",
    "> The scikit-learn (sklearn) library has implemented many different classifiers (see below). Which classifier performs best? (hint: use `plot_decision_boundary` to observe the decision boundary for each classifier, and `train_test_split` to create a training and test set with 50% test size)\n",
    ">\n",
    "\n",
    "| Classifier   | Function name     |\n",
    "|-------------------------------|---------------------------------|\n",
    "| Linear bayes classifier       | LinearDiscriminantAnalysis()    |\n",
    "| Quadratic bayes classifier    | QuadraticDiscriminantAnalysis() |\n",
    "| k-Nearest neighbor classifier | KNeighborsClassifier()          |\n",
    "| Support Vector Machine        | SVC(probability=True)           |\n",
    "| Nearest mean classifier       | NearestCentroid()               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7Ut9-N32_gr"
   },
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "# Your code here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ueOXzrJL3AOf"
   },
   "source": [
    "## 5. Training a classifier: effect of training set size\n",
    "\n",
    "The code below will generate a two-dimensional Gaussian dataset with 20 samples, 2 features. Mean equals to (1,1) and (2,2), and variance equals to 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gaussian_shaped = datasets.make_blobs(n_samples=20, n_features=2, centers=[[1, 1], [2, 2]], cluster_std=[1, 2])\n",
    "data = Gaussian_shaped[0]\n",
    "labels =  Gaussian_shaped[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 5a\n",
    "\n",
    "> Train the k-Nearest neighbor classifier on the dataset obtained above (hint: use KNeighborsClassifier with 3 neighbors, and use fit method to train the classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "# Your code here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 5b\n",
    "\n",
    "> Now create a larger dataset with 1000 samples and use it to test the classifier trained in a. What is your error rate now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "# Your code here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 5c\n",
    "\n",
    "> Generate a new Gaussian, two-dimensional dataset with 500 samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "# Your code here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 5d\n",
    "\n",
    "> Train the k-Nearest neighbor classifier with 3 neighbors on this new set and then test it on the set with 1000 samples in the previous exercise. What is your error rate?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "# Your code here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 5e\n",
    "\n",
    "> Can you explain the different error rates obtained?\n",
    ">\n",
    "> Hint: Apply the `learning_curve` function given in `LST_Functions` to see the k-Nearest neighbor error on this dataset, using training set of different sizes\n",
    ">\n",
    "> Hint: The `learning_curve` function takes a while, because it has to retrain many times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7hvCSH9KM6f"
   },
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "learning_curve()  # Use this function!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ra_Axktf3yHZ"
   },
   "source": [
    "## 6. Marker gene selection\n",
    "\n",
    "The code below loads the \"Genesdata.pkl\" file, which contains two datasets: (X_train, y_train) and (X_test, y_test). Some genes in these datasets are good markers, while some contain less information about the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "error",
     "timestamp": 1709817311598,
     "user": {
      "displayName": "Timo Verlaan",
      "userId": "18117607379973306750"
     },
     "user_tz": -60
    },
    "id": "lPtdR-qzLbVS",
    "outputId": "d4ab289b-4276-47e5-dcf2-3bce8279fa16"
   },
   "outputs": [],
   "source": [
    "# Load the genes dataset\n",
    "with open('Genesdata.pkl', 'rb') as f:\n",
    "  datadict = pickle.load(f)\n",
    "\n",
    "X_train = datadict['X_train']\n",
    "X_test = datadict['X_test']\n",
    "y_train = datadict['y_train']\n",
    "y_test = datadict['y_test']\n",
    "del datadict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1qRdXRhK2E4"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 6a\n",
    "\n",
    "> Employ a criterion such as the t-statistic to evaluate a gene\u2019s predictive power, using the training set. (hint: `X_train` contains the data, and the class labels are stored in `y_train`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "# Your code here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 6b\n",
    "\n",
    "> Identify the two best genes by sorting the t-statistic. (hint: remember to look at the absolute value of t-statistic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "# Your code here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 6c\n",
    "\n",
    "> Retain the two top features/genes and train a classifier using the training dataset `(X_train, y_train)`. Then test the trained classifier using the test dataset `(X_test, y_test)` (hint: using `LinearDiscriminantAnalysis`, fit and predict functions). What is the error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "# Your code here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 6d\n",
    "\n",
    "> Use `plot_decision_boundary` to visualize the two selected features for each dataset separately. Are these features the best overall separators for the two classes? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "# Your code here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SpGcIRl5Qhj"
   },
   "source": [
    "## Exercise 7 (Basic Cross-validation)\n",
    "\n",
    "Generate a dataset using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NCF4CAY6L6AS"
   },
   "outputs": [],
   "source": [
    "Banana_shaped = datasets.make_moons(n_samples=1000, noise=0.1)\n",
    "data = Banana_shaped[0]\n",
    "labels = Banana_shaped[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hJhRmyPMC3u"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### \u270f Exercise 7\n",
    "\n",
    "> Use the Python function `StratifiedKFold` to create 3-folds. Use `split` to get training and test fold indices. Split the data into test and training sets using these indices (see code below). For each fold, train three classifiers including linear bayes, SVM and Random Forest and compare their performances. Which one performs better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1706707140559,
     "user": {
      "displayName": "Timo Verlaan",
      "userId": "18117607379973306750"
     },
     "user_tz": -60
    },
    "id": "A-P9M76oMJmC",
    "outputId": "c37da2d0-9144-4bcc-ee17-dee76139180a"
   },
   "outputs": [],
   "source": [
    "# Use one classifier at a time\n",
    "Classifier = LinearDiscriminantAnalysis()\n",
    "Classifier = SVC()\n",
    "Classifier = RandomForestClassifier(n_estimators=3)\n",
    "\n",
    "# Create cross validation and use for loop to cover all folds\n",
    "CV = StratifiedKFold(n_splits=3)\n",
    "\n",
    "# SOLUTION:\n",
    "for train_ind, test_ind in CV.split(data, labels):\n",
    "  ??? # Replace the ??? with the correct code\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1F5DW1ZH2GAjP-rqTjlplSxBCW_LboVCF",
     "timestamp": 1705942892540
    }
   ]
  },
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}